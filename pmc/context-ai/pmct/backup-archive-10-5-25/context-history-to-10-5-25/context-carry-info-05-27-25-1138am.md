# Development Context & Operational Priorities
**Date:** 2025-05-27 11:38 PST
**Project:** Aplio Design System Modernization (aplio-mod-1) & Project Memory Core (PMC)
**Context Version:** 3.0.0

## Introduction

This context document addresses two integrated projects that operate in tandem:

1. **Aplio Design System Modernization (aplio-mod-1)**: A comprehensive modernization project converting the legacy Aplio Design System to leverage Next.js 14's App Router architecture and TypeScript. The project focuses on the Home-4 template implementation while preserving visual fidelity and enhancing developer experience.

2. **Project Memory Core (PMC)**: A structured task management and context retention system that manages the development of the Aplio project. PMC provides methodical task tracking, context preservation, and implementation guidance through its command-line interface and document-based workflow.

These projects are deliberately interconnected - PMC requires a real-world development project to refine its capabilities, while Aplio benefits from PMC's structured approach to development. Depending on current priorities, work may focus on either advancing the Aplio Design System implementation or enhancing the PMC tooling itself.

## Current Focus

### Active Development Focus

# Phase 6D: CRITICAL DEFECT - False Positive Testing Logic Architecture Fix Specification

## CRITICAL ISSUE SUMMARY

**This is the HIGHEST PRIORITY ARCHITECTURAL FIX** - The current testing infrastructure has a fundamentally flawed test validation logic that creates systematic false positives, representing the most critical architectural defect in the system. The testing logic only validates whether the LLM Vision API executed successfully and returned data, but **COMPLETELY IGNORES** whether the component actually passed validation (`analysisResult.validation.passed`). This means broken components with serious rendering issues are incorrectly reported as successful.

## ROOT CAUSE ANALYSIS - CRITICAL FLAW

**Primary Cause**: Line 409 in `C:\Users\james\Master\BrightHub\Build\APSD-runs\aplio-27-a1-c\aplio-modern-1\test\vision-test-suite.js` contains critically flawed logic
**Secondary Cause**: No distinction between infrastructure success and component validation success

### Affected Files
- `C:\Users\james\Master\BrightHub\Build\APSD-runs\aplio-27-a1-c\aplio-modern-1\test\vision-test-suite.js` - Line 409 contains the critical flaw
- All test reports generated by this logic - Contains false positive results
- `C:\Users\james\Master\BrightHub\Build\APSD-runs\aplio-27-a1-c\aplio-modern-1\test\comprehensive-testing-report.md` - Contains incorrect validation results

### EVIDENCE OF FALSE POSITIVE

**DashboardStats Component Analysis** (Documented example):
- `analysisResult.componentClassification`: ✅ "Server Component: DashboardStats" (API worked)
- `analysisResult.validation.passed`: ❌ **false** (Component FAILED validation)
- `analysisResult.validation.confidence`: 0.95 (High confidence in the FAILURE)
- **Critical Issues Found**: React rendering errors, Promise objects being rendered
- **Current Test Result**: ❌ **INCORRECTLY MARKED AS PASSED**

### TECHNICAL DETAILS OF THE FLAW

**Current Flawed Logic (Line 409)**:
```javascript
const testPassed = analysisResult && analysisResult.componentClassification;
```

**What this checks**:
1. ✅ API call succeeded (analysisResult exists)
2. ✅ Component was classified (componentClassification exists)

**What this IGNORES**:
1. ❌ Whether `validation.passed` is true or false
2. ❌ Actual component rendering quality
3. ❌ Specific validation issues found
4. ❌ Component functionality assessment

## DETAILED SOLUTION SPECIFICATION

### Step 1: IMMEDIATE FIX - Correct Line 409 (CRITICAL PRIORITY)

**What to do**: Fix the false positive logic immediately before any other work
**When to do it**: FIRST ACTION - This must be done before trusting any test results
**How to do it**: Locate and replace the flawed logic in `C:\Users\james\Master\BrightHub\Build\APSD-runs\aplio-27-a1-c\aplio-modern-1\test\vision-test-suite.js`

**FIND THIS EXACT LINE (approximately line 409)**:
```javascript
const testPassed = analysisResult && analysisResult.componentClassification;
```

**REPLACE WITH CORRECTED LOGIC**:
```javascript
// CORRECTED LOGIC: Actually check if component validation passed
const testPassed = analysisResult && 
                  analysisResult.componentClassification && 
                  analysisResult.validation && 
                  analysisResult.validation.passed === true;
```

**Code Usage Context**: This is the core validation logic that determines whether a component passes or fails testing. The corrected logic ensures that only components that actually passed LLM vision validation are marked as successful.

### Step 2: Implement Infrastructure vs Component Validation Separation

**What to do**: Create separate validation functions to distinguish between API functionality and component quality
**When to do it**: Immediately after fixing the main logic flaw
**How to do it**: Add these functions to `C:\Users\james\Master\BrightHub\Build\APSD-runs\aplio-27-a1-c\aplio-modern-1\test\vision-test-suite.js`

```javascript
// aplio-modern-1/test/vision-test-suite.js
// USAGE: Add these functions to separate infrastructure tests from component validation
// WHEN: Use during test execution to properly categorize different types of success/failure
// HOW: Call both functions and report results separately

/**
 * USAGE: Tests if the LLM Vision API and infrastructure are working correctly
 * WHEN: Use to verify that testing tools are functional (separate from component quality)
 * HOW: Returns true if API calls succeed and return classification data
 */
function validateInfrastructure(analysisResult) {
  return analysisResult && 
         analysisResult.componentClassification && 
         typeof analysisResult.componentClassification === 'string';
}

/**
 * USAGE: Tests if the component actually rendered correctly and passed validation
 * WHEN: Use to verify actual component implementation quality
 * HOW: Returns true only if validation.passed is explicitly true
 */
function validateComponentQuality(analysisResult) {
  return analysisResult && 
         analysisResult.validation && 
         analysisResult.validation.passed === true;
}

/**
 * USAGE: Provides comprehensive validation with both infrastructure and component checks
 * WHEN: Use as the main validation function for complete test assessment
 * HOW: Returns detailed status for both infrastructure and component validation
 */
function comprehensiveValidation(analysisResult) {
  const infrastructureWorking = validateInfrastructure(analysisResult);
  const componentValid = validateComponentQuality(analysisResult);
  
  return {
    infrastructureStatus: infrastructureWorking ? 'WORKING' : 'FAILED',
    componentStatus: componentValid ? 'PASSED' : 'FAILED',
    overallResult: infrastructureWorking && componentValid ? 'SUCCESS' : 'FAILURE',
    analysisDetails: {
      classification: analysisResult?.componentClassification || 'None',
      validationPassed: analysisResult?.validation?.passed || false,
      confidence: analysisResult?.validation?.confidence || 0,
      issues: analysisResult?.validation?.issues || [],
      hasRenderingErrors: analysisResult?.validation?.issues?.some(issue => 
        issue.toLowerCase().includes('rendering') ||
        issue.toLowerCase().includes('promise') ||
        issue.toLowerCase().includes('error')
      ) || false
    }
  };
}
```

**Code Usage Context**: These functions provide clear separation between testing the tools themselves vs. testing component quality. This prevents infrastructure success from masking component failures.

### Step 3: Implement Confidence Threshold Validation

**What to do**: Add confidence threshold checking to prevent low-confidence false positives
**When to do it**: After implementing the basic validation separation
**How to do it**: Add confidence validation to the testing logic

```javascript
// aplio-modern-1/test/vision-test-suite.js
// USAGE: Validates that analysis results meet minimum confidence requirements
// WHEN: Use during component validation to ensure reliable results
// HOW: Set appropriate confidence threshold based on testing requirements

/**
 * USAGE: Validates analysis confidence meets minimum threshold requirements
 * WHEN: Use to filter out low-confidence analysis results that may be unreliable
 * HOW: Set confidenceThreshold based on testing accuracy requirements (0.8 recommended)
 */
function validateConfidenceThreshold(analysisResult, confidenceThreshold = 0.8) {
  if (!analysisResult?.validation?.confidence) {
    return {
      valid: false,
      reason: 'No confidence score available',
      confidence: 0
    };
  }

  const confidence = analysisResult.validation.confidence;
  const meetsThreshold = confidence >= confidenceThreshold;

  return {
    valid: meetsThreshold,
    reason: meetsThreshold ? 
      `Confidence ${confidence} meets threshold ${confidenceThreshold}` :
      `Confidence ${confidence} below threshold ${confidenceThreshold}`,
    confidence: confidence
  };
}

/**
 * USAGE: Complete validation including confidence threshold checking
 * WHEN: Use as the final validation function for test results
 * HOW: Combines component validation with confidence requirements
 */
function finalValidation(analysisResult, confidenceThreshold = 0.8) {
  const componentValidation = validateComponentQuality(analysisResult);
  const confidenceValidation = validateConfidenceThreshold(analysisResult, confidenceThreshold);
  
  const finalResult = componentValidation && confidenceValidation.valid;
  
  return {
    passed: finalResult,
    componentValid: componentValidation,
    confidenceValid: confidenceValidation.valid,
    confidence: confidenceValidation.confidence,
    details: {
      componentStatus: componentValidation ? 'PASSED' : 'FAILED',
      confidenceStatus: confidenceValidation.valid ? 'SUFFICIENT' : 'INSUFFICIENT',
      confidenceReason: confidenceValidation.reason,
      issues: analysisResult?.validation?.issues || []
    }
  };
}
```

**Code Usage Context**: Confidence threshold validation prevents accepting analysis results that have low confidence scores, which could indicate unreliable analysis or edge cases that need human review.

### Step 4: Update Test Reporting to Show Real Results

**What to do**: Modify test reporting to accurately reflect component validation status
**When to do it**: After implementing the corrected validation logic
**How to do it**: Update the test reporting functions to use new validation methods

```javascript
// aplio-modern-1/test/vision-test-suite.js
// USAGE: Generates accurate test reports showing true component validation status
// WHEN: Called during test report generation to provide correct status information
// HOW: Replace existing report generation with this accurate reporting

/**
 * USAGE: Generates detailed test report with accurate pass/fail status
 * WHEN: Use after running vision analysis to create comprehensive reports
 * HOW: Pass analysis results array to generate formatted report
 */
function generateAccurateTestReport(analysisResults, confidenceThreshold = 0.8) {
  const reportData = {
    timestamp: new Date().toISOString(),
    totalTests: analysisResults.length,
    infrastructureFailures: 0,
    componentFailures: 0,
    confidenceFailures: 0,
    actualPasses: 0,
    falsePositives: 0,
    detailedResults: []
  };

  analysisResults.forEach((result, index) => {
    const validation = finalValidation(result.analysisResult, confidenceThreshold);
    const oldLogic = result.analysisResult && result.analysisResult.componentClassification; // Old flawed logic
    
    // Detect false positives (old logic says pass, new logic says fail)
    const isFalsePositive = oldLogic && !validation.passed;
    if (isFalsePositive) {
      reportData.falsePositives++;
    }

    if (!validateInfrastructure(result.analysisResult)) {
      reportData.infrastructureFailures++;
    } else if (!validation.componentValid) {
      reportData.componentFailures++;
    } else if (!validation.confidenceValid) {
      reportData.confidenceFailures++;
    } else {
      reportData.actualPasses++;
    }

    reportData.detailedResults.push({
      testIndex: index + 1,
      component: result.component || 'Unknown',
      screenshot: result.screenshot || 'Unknown',
      infrastructureStatus: validateInfrastructure(result.analysisResult) ? 'WORKING' : 'FAILED',
      componentStatus: validation.componentValid ? 'PASSED' : 'FAILED',
      confidenceStatus: validation.confidenceValid ? 'SUFFICIENT' : 'INSUFFICIENT',
      overallResult: validation.passed ? 'PASS' : 'FAIL',
      oldLogicResult: oldLogic ? 'PASS' : 'FAIL',
      falsePositive: isFalsePositive,
      confidence: validation.confidence,
      issues: validation.details.issues,
      classification: result.analysisResult?.componentClassification || 'None'
    });
  });

  return reportData;
}

/**
 * USAGE: Creates markdown report showing corrected test results
 * WHEN: Use to generate human-readable reports with accurate results
 * HOW: Pass report data from generateAccurateTestReport to create markdown
 */
function createMarkdownReport(reportData) {
  const successRate = reportData.totalTests > 0 ? 
    ((reportData.actualPasses / reportData.totalTests) * 100).toFixed(1) : 0;

  let markdown = `# Corrected Vision Analysis Test Report

## Summary
- **Total Tests**: ${reportData.totalTests}
- **Actual Passes**: ${reportData.actualPasses}
- **Component Failures**: ${reportData.componentFailures}
- **Infrastructure Failures**: ${reportData.infrastructureFailures}
- **Confidence Failures**: ${reportData.confidenceFailures}
- **False Positives Detected**: ${reportData.falsePositives}
- **Corrected Success Rate**: ${successRate}%

## Critical Issues Found
${reportData.falsePositives > 0 ? `
⚠️ **${reportData.falsePositives} False Positive(s) Detected**
These components were incorrectly marked as passing by the old logic but actually failed validation.
` : '✅ No false positives detected'}

## Detailed Results

`;

  reportData.detailedResults.forEach(result => {
    const statusIcon = result.overallResult === 'PASS' ? '✅' : '❌';
    const falsePositiveWarning = result.falsePositive ? ' ⚠️ **FALSE POSITIVE**' : '';
    
    markdown += `### ${statusIcon} ${result.component}${falsePositiveWarning}

- **Overall Result**: ${result.overallResult}
- **Component Status**: ${result.componentStatus}
- **Infrastructure Status**: ${result.infrastructureStatus}
- **Confidence**: ${result.confidence} (${result.confidenceStatus})
- **Classification**: ${result.classification}
${result.oldLogicResult !== result.overallResult ? 
  `- **Old Logic Would Have**: ${result.oldLogicResult} (INCORRECT)` : ''}
${result.issues.length > 0 ? `
- **Issues Found**:
${result.issues.map(issue => `  - ${issue}`).join('\n')}
` : ''}

`;
  });

  markdown += `
## Recommendations

${reportData.componentFailures > 0 ? `
### Component Issues (${reportData.componentFailures} failures)
- Review components marked as FAILED in the detailed results
- Address specific validation issues listed for each component
- Re-run analysis after fixes to confirm resolution
` : ''}

${reportData.falsePositives > 0 ? `
### False Positive Investigation (${reportData.falsePositives} detected)
- These components need immediate attention as they were incorrectly passing
- Review the validation issues for each false positive component
- Implement fixes for rendering errors and validation failures
` : ''}

Report Generated: ${reportData.timestamp}
Using Corrected Validation Logic
`;

  return markdown;
}
```

**Code Usage Context**: This reporting system provides accurate visibility into test results, clearly distinguishing between infrastructure issues and component validation failures, and highlighting any false positives that would have been missed by the old logic.

### Step 5: Add Specific Rendering Error Detection

**What to do**: Implement detection for common React rendering errors that cause validation failures
**When to do it**: After implementing the basic corrected validation
**How to do it**: Add specific error pattern detection

```javascript
// aplio-modern-1/test/vision-test-suite.js
// USAGE: Detects specific types of React rendering errors that commonly cause failures
// WHEN: Use during component analysis to identify and categorize common problems
// HOW: Call this function with analysis results to get specific error classifications

/**
 * USAGE: Detects and categorizes common React rendering errors
 * WHEN: Use during component validation to identify specific types of issues
 * HOW: Pass analysis result to get categorized error information
 */
function detectRenderingErrors(analysisResult) {
  const issues = analysisResult?.validation?.issues || [];
  
  const errorPatterns = {
    promiseRendering: {
      pattern: /promise.*render|render.*promise/i,
      severity: 'CRITICAL',
      description: 'Promise objects being rendered instead of resolved values'
    },
    hydrationMismatch: {
      pattern: /hydration.*mismatch|mismatch.*hydration/i,
      severity: 'HIGH',
      description: 'Server and client rendering mismatch'
    },
    undefinedProps: {
      pattern: /undefined.*prop|prop.*undefined/i,
      severity: 'MEDIUM',
      description: 'Undefined props causing rendering issues'
    },
    lifecycleErrors: {
      pattern: /lifecycle.*error|useEffect.*error/i,
      severity: 'HIGH',
      description: 'Component lifecycle or hook errors'
    },
    keyErrors: {
      pattern: /key.*warning|duplicate.*key/i,
      severity: 'LOW',
      description: 'Missing or duplicate React keys'
    }
  };

  const detectedErrors = [];
  
  issues.forEach(issue => {
    Object.entries(errorPatterns).forEach(([errorType, pattern]) => {
      if (pattern.pattern.test(issue)) {
        detectedErrors.push({
          type: errorType,
          severity: pattern.severity,
          description: pattern.description,
          originalIssue: issue
        });
      }
    });
  });

  return {
    hasRenderingErrors: detectedErrors.length > 0,
    errorCount: detectedErrors.length,
    criticalErrors: detectedErrors.filter(e => e.severity === 'CRITICAL'),
    highSeverityErrors: detectedErrors.filter(e => e.severity === 'HIGH'),
    allDetectedErrors: detectedErrors,
    summary: detectedErrors.length > 0 ? 
      `${detectedErrors.length} rendering errors detected (${detectedErrors.filter(e => e.severity === 'CRITICAL').length} critical)` :
      'No specific rendering errors detected'
  };
}
```

**Code Usage Context**: This function helps identify specific types of rendering issues that commonly cause component validation failures, making it easier to diagnose and fix problems.

### Step 6: IMMEDIATE VALIDATION TESTING

**What to do**: Test the corrected logic immediately to verify it works correctly
**When to do it**: IMMEDIATELY after implementing the logic fix
**How to do it**: Run these validation tests to confirm the fix

```bash
# CRITICAL TEST 1: Verify the corrected logic exists
# USAGE: Confirms that the flawed logic has been replaced
# WHEN: Run immediately after making the Line 409 fix
grep -n "validation.passed === true" aplio-modern-1/test/vision-test-suite.js

# CRITICAL TEST 2: Verify old flawed logic is removed
# USAGE: Confirms that the false positive logic is no longer present
# WHEN: Run to ensure old logic is completely removed
grep -n "analysisResult && analysisResult.componentClassification" aplio-modern-1/test/vision-test-suite.js

# CRITICAL TEST 3: Re-run all existing tests with corrected logic
# USAGE: Validates which components actually pass vs. fail with correct logic
# WHEN: After implementing corrected validation logic
npm run test:vision:batch

# CRITICAL TEST 4: Generate corrected test report
# USAGE: Creates accurate test report showing real component validation results
# WHEN: After rerunning tests with corrected logic
npm run test:vision:report

# CRITICAL TEST 5: Compare old vs new results
# USAGE: Identifies which components were false positives
# WHEN: After generating new reports to compare against old results
diff aplio-modern-1/test/reports/old-comprehensive-testing-report.md aplio-modern-1/test/reports/corrected-testing-report.md
```

**Expected Results**:
- Test 1: Should show the corrected validation logic is present
- Test 2: Should show NO results (old flawed logic removed)
- Test 3: Should reveal components that actually fail (like DashboardStats)
- Test 4: Should generate report with accurate pass/fail status
- Test 5: Should show differences between false positive and accurate results

## Documentation Updates Required

After implementing this fix, update `pmc/docs/stm-5a/testing-system-operations-tutorial-v6.md`:

### Section 4.4: Test Validation Logic
- Document the corrected validation logic and why it's critical
- Explain the difference between infrastructure success and component validation
- Add examples of false positive detection and prevention

### Section 5.1: Interpreting Test Results  
- Update result interpretation to use corrected validation criteria
- Add guidance on distinguishing infrastructure vs. component failures
- Include troubleshooting guide for common validation failures

### Section 6.3: Critical Testing Issues
- Create new section documenting the false positive problem and solution
- Add validation checklist to prevent future false positive logic
- Include examples of proper vs. improper validation logic

## Phase 6 Architecture Integration

### CRITICAL Dependencies on Other Phase 6 Fixes
This fix is **THE FOUNDATION** of Phase 6 Architecture Upgrade and MUST be implemented FIRST:
- **All other Phase 6 fixes depend on this fix** being implemented first
- **Phase 6A**: Coverage results cannot be trusted until validation logic is corrected
- **Phase 6B**: CLI commands will use corrected validation logic
- **Phase 6C**: Framework integration will implement corrected validation as core logic

### Architecture Upgrade Dependencies
- **Corrected validation logic** becomes the foundation for all testing validation in the new architecture
- **Framework validation phase** will use this corrected logic as its core validation method
- **Reporting accuracy** across all testing phases depends on this fix
- **Tool coordination** requires reliable validation to make decisions about pass/fail status

### Impact on Existing Tests
- **ALL existing test reports are invalidated** and must be regenerated
- **Components previously marked as "passing" may now fail** (this is correct)
- **Test success metrics will be more accurate** but likely lower

### Critical Timeline
This fix must be implemented **IMMEDIATELY** before any other testing work can be trusted. Any testing done with the false positive logic is fundamentally unreliable.

## Success Criteria

### CRITICAL Validation
1. **Line 409 logic corrected** - No longer ignores validation.passed
2. **False positives eliminated** - Components with validation.passed=false now fail tests
3. **DashboardStats component now fails** - The documented false positive case now correctly fails
4. **All test reports regenerated** with accurate results

### Functional Validation
1. **Infrastructure vs. component validation separated** clearly
2. **Confidence thresholds enforced** properly
3. **Specific error detection** identifies common rendering problems
4. **Test reports show real status** instead of false positives

## Risk Mitigation

### Immediate Risks
- **All current test results are unreliable** until this fix is applied
- **Components may be shipped with serious issues** due to false positive masking
- **Development time wasted** on components that appear to work but don't

### Mitigation Actions
- **STOP all testing work** until this fix is implemented
- **Mark all existing test reports as "UNRELIABLE"** 
- **Regenerate all test reports** after implementing the fix
- **Review all components marked as "passing"** to verify they actually work


## Project Reference Guide
REFERENCE MATERIALS
Everything below this line is supporting information only. Do NOT select the current task focus from this section.

### Aplio Design System Modernization Project

#### Project Overview
This project aims to transform the existing JavaScript-based Aplio theme into a modern TypeScript-powered Next.js 14 platform. The project specifically focuses on migrating the Home 4 template (https://js-aplio-6.vercel.app/home-4) as the flagship demonstration while preserving Aplio's premium design aesthetics from the existing design system in `/aplio-legacy/`.

#### Key Documents
1. Seed Story: `pmc/product/00-aplio-mod-1-seed-story.md`
2. Project Overview: `pmc/product/00-aplio-mod-1-seed-narrative.md`
3. Raw Data: `pmc/product/_seeds/00-narrative-raw_data-ts-14-v3.md`

#### Project Objectives

##### Primary Goals
1. Migrate Home 4 template to Next.js 14 App Router architecture
2. Preserve exact design elements from `/aplio-legacy/`
3. Implement TypeScript with full type safety
4. Maintain premium design quality and animations

##### Technical Requirements
1. Next.js 14 App Router implementation
2. Complete TypeScript migration
3. Modern component architecture
4. Performance optimization

##### Design Requirements
1. Exact preservation of design elements from `/aplio-legacy/`
2. Maintenance of animation quality
3. Responsive behavior preservation
4. Professional template implementation

### Project Memory Core (PMC) System

#### Core Functionality
Everything in this section is supporting information only. Do NOT select the current task focus from this section.
PMC is a structured modern software development task management and context retention system built around the the main active task file as its central operational component. PMC is product agnostic. In this instance we are using it to code the Aplio Design System Modernization (aplio-mod-1) system described above. The system provides:

1. **Context Locality**: Instructions and context are kept directly alongside their relevant tasks
2. **Structured Checkpoints**: Regular token-based checks prevent context drift
3. **Directive Approach**: Clear commands and instructions with explicit timing requirements
4. **Task-Centric Documentation**: Single source of truth for task implementation

#### Commands

The driver for most PMC commands are in:
`pmc/bin/aplio-agent-cli.js`

The code for most PMC commands are contained within:
- The original context manager script: `pmc/system/management/context-manager.js`
- The next context manager script: `pmc/system/management/context-manager-v2.js` (created when the original got too large)

Here are some important PMC commands:

##### Start Task
```bash
node pmc/bin/aplio-agent-cli.js start-task "T-EXAMPLE.1.1"
```

##### Add Structured Task Approaches
```bash
node pmc/bin/aplio-agent-cli.js task-approach
```

##### Update Element Status
```bash
node pmc/bin/aplio-agent-cli.js update-element-status "T-EXAMPLE.1.2:ELE-1" "Complete"
```

##### Task Completion
```bash
node pmc/bin/aplio-agent-cli.js complete-task "T-EXAMPLE.1.3"
```

##### Context Carryover for New Chat Windows
```bash
node system/management/carryover-command.js
```

#### Project Structure
```
aplio-legacy/ (legacy system)
aplio-modern-1/ (new system)
pmc/ (PMC system)
├── bin/
├── core/
├── system/
│   ├── management/
│   └── templates/
└── product/
```

### Additional Resources

#### Key URLs

#### Important Directories